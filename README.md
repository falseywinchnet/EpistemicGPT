# > An Epistemic Generative Pretrained Transformer Framework.
- we present an experimental alternative to softmax attention.
- we independently derive and implement some mechanisms later discussed in Towards Transformer Understanding.
- we produced our own MLP customization wih a nonlinearity that boosts performance.
- We produce some experimental models, modules, and tricks.
- We produce some useful visualization aids and a program to show the model performance live.
- we collected papers and ideas and concepts here.


FOCUS on LLM_KNOWLEDGE.pdf and NewGPT.ipynb before accessing anything else!

# Dedication
- This repository is a goldmine to completely rework your epistemic depth on large language models.
- We explain all of our efforts as being only possible for the glory and grace of god,
- and we dedicate them in the name of christ, our heavenly king and savior, to the public domain.

- We do accept donations or paychecks in the interest of continued scholarly efforts,
and declare ourselves un-necessarily impoverished and spartan. 



